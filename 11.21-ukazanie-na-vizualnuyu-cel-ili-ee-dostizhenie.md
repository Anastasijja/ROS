# 11.21 Указание на визуальную цель или ее достижение

Для более понятного примера использования обратной кинематики для позиционирования конечного эффектора рассмотрим узел arm\_tracker.py, находящийся в каталоге rbx2\_arm\_nav/nodes. Этот сценарий работает по аналогии с узлом head\_tracker.py, который мы рассматривали в главе об отслеживании 3D головы. Только на этот раз вместо того, чтобы вращать камеру, чтобы посмотреть на объект, мы будем двигать руку, чтобы робот эффективно "указывал" в сторону цели или достигал ее.

Сценарий arm\_tracker.py должен быть достаточно простым, учитывая комментарии и его сходство с отслеживанием головы, поэтому мы не будем говорить о нем здесь. Основная стратегия состоит в том, чтобы подписаться на тему/target\_pose, а затем использовать обратную кинематику руки, чтобы позиционировать конечный эффект так, чтобы он был как можно ближе к цели. В то же время захват поддерживается относительно горизонтальным для облегчения захвата в том случае, когда это является конечной целью.

Если целевой объект перемещается, вычисляется новое IK-решение, и рука соответствующим образом перемещает захват.

Одно различие между отслеживанием головы и отслеживанием руки состоит в том, что мы не можем обновить положение руки так же быстро, как углы панорамирования и наклона головы. Причина в том, что вычисление IK-решений занимает значительное количество времени, как мы могли видеть в предыдущем разделе. Таким образом, сценарий проверяет, не продвинулась ли цель достаточно далеко от своей последней позиции, чтобы сделать возможным повторное перемещение руки. Если цель перемещается слишком далеко за пределы досягаемости, робот опускает руку в положение покоя. Хотя сценарий должен работать, используя решающие устройство KDL ИКА MoveIt!, мы можем получить более быстрые и надежные результаты, используя таможенное решающее устройство IK для руки робота, о которой говорится в последним разделом этой главы.

Программа arm\_tracker.py также иллюстрирует, как использовать библиотеку ROS tf для преобразования целевых позиций между различными опорными кадрами. Например, чтобы определить направление в пространстве, на которое должна ориентироваться рука, мы можем преобразовать позу цели относительно опорной рамы, прикрепленной к плечу робота, а не базовой рамы. Затем мы размещаем захват где-то вдоль линии, соединяющей плечо с мишенью, и на расстоянии, равном расстоянию от мишени до плеча или длине руки, в зависимости от того, какое значение меньше.

Давайте проверим узел отслеживания рук с помощью симулятора ArbotiX и 3D цели, которую мы использовали с отслеживанием головы. Если он еще не запущен, приведите в действие симуляцию однорукого робота Pi:

```text
$ roslaunch rbx2_bringup pi_robot_with_gripper.launch sim:=true
```

И если у вас еще не запущен файл move\_group.launch для Робота Pi, запустите его:

```text
$ roslaunch pi_robot_moveit_config move_group.launch
```

Теперь вызовите RViz с помощью конфигурационного файла fake\_target.rviz из пакета rbx2\_utils:

```text
$ rosrun rviz rviz -d `rospack find rbx2_utils`/fake_target.rviz
```

Далее запустите 3D цель с относительно низкой скоростью движения:

```text
$ roslaunch rbx2_utils pub_3d_target.launch speed:=0.2
```

Желтый воздушный шар должен появиться в RViz и медленно двигаться перед роботом. Напомним, что узел pub\_3d\_target.py публикует позу аэростата на теме the/target\_pose.

Просто для развлечения, давайте поднимем головной отслеживающий узел в режиме симуляции, чтобы голова Pi отслеживала цель:

```text
$ roslaunch rbx2_dynamixels head_tracker.launch sim:=true
```

Через несколько мгновений голова должна начать отслеживать воздушный шар в RViz.

Наконец, запустите узел arm\_tracker.py:

```text
$ rosrun rbx2_arm_nav arm_tracker.py
```

Если все идет хорошо, Pi должен периодически обновлять положение его руки так, чтобы захват достигал шара.

